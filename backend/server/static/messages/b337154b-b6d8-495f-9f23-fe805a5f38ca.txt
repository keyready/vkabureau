import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, roc_curve, auc
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np
import xgboost as xgb
from sklearn.tree import plot_tree

# 1. Считываем файл Excel
file_path = input("Введите путь к файлу Excel: ")

# Проверка на наличие файла
try:
    data = pd.read_excel(file_path)
    print(f"Количество строк: {data.shape[0]}")
    print(f"Количество столбцов: {data.shape[1]}")
except Exception as e:
    print(f"Ошибка при загрузке файла: {e}")
    exit()

# 2. Вводим номера столбцов для признаков и целевой переменной
features_index = input("Введите номера столбцов признаков, разделенные запятыми: ").split(',')
target_index = input("Введите номер столбца целевой переменной: ")

# Преобразуем индексы в целые числа и извлекаем данные
try:
    features_index = [int(i) for i in features_index]
    X = data.iloc[:, features_index]
    y = data.iloc[:, int(target_index)]
except (ValueError, IndexError) as e:
    print(f"Ошибка при выборе столбцов: {e}")
    exit()

# 3. Проверяем наличие пропусков
missing_info = X.isnull().sum()  # Количество пропусков в каждом столбце
total_missing = missing_info[missing_info > 0].sum()  # Общее количество пропусков

if total_missing > 0:
    print(f"Общее количество пропущенных значений до заполнения: {total_missing}")
    X_missing = X[X.isnull().any(axis=1)]  # Выбираем строки с пропусками
    X_no_missing = X.dropna()  # Данные без пропусков
    y_no_missing = y[X_no_missing.index]  # Соответствующие метки

    # Обучаем модель XGBoost для заполнения пропусков
    dtrain = xgb.DMatrix(X_no_missing, label=y_no_missing)
    params = {
        'objective': 'reg:squarederror',  # Для регрессии
        'max_depth': 3,
        'eta': 0.1,
        'eval_metric': 'rmse'
    }
    model_xgb = xgb.train(params, dtrain)

    # Заполняем пропуски
    total_replacements = 0
    for col in X_missing.columns:
        if X[col].isnull().any():
            # Прогнозируем значения для текущего столбца
            missing_data = X_missing.drop(columns=[col])
            dmissing = xgb.DMatrix(missing_data)
            predictions = model_xgb.predict(dmissing)
            total_replacements += predictions.size  # Увеличиваем счетчик замен
            X.loc[X_missing.index, col] = predictions

    # Проверка на наличие пустых значений после заполнения
    if X.isnull().any().any():
        print("Ошибка: некоторые пропуски не были заполнены.")
        exit()

    # Обновляем информацию о пропусках
    missing_info_after = X.isnull().sum()
    total_missing_after = missing_info_after[missing_info_after > 0].sum()

    print(f"Общее количество пропущенных значений после заполнения: {total_missing_after}")
    print(f"Количество замененных значений: {total_replacements}")
else:
    print("Пропущенные значения отсутствуют.")

# Проверка распределения классов
print("Распределение классов в целевой переменной:")
print(y.value_counts())

# 4. Масштабируем данные
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# 5. Делим данные на обучающую, валидационную и тестовую выборки
train_size = float(input("Введите долю обучающей выборки (например, 0.6 для 60%): "))
val_size = float(input("Введите долю валидационной выборки (например, 0.2 для 20%): "))
test_size = 1 - train_size - val_size

X_train, X_temp, y_train, y_temp = train_test_split(X_scaled, y, test_size=test_size, random_state=42)
X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)

# 6. Запрашиваем параметры для модели
n_estimators = int(input("Введите количество деревьев в лесу: "))
max_depth = int(input("Введите максимальную глубину дерева: "))
min_samples_split = int(input("Введите минимальное количество образцов для разбиения узла: "))
max_samples = float(input("Введите максимальное количество образцов в дереве (например, 0.8 для 80%): "))

# 7. Обучаем модель Случайный лес
model = RandomForestClassifier(
    n_estimators=n_estimators,
    max_depth=max_depth,
    min_samples_split=min_samples_split,
    max_samples=max_samples,
    random_state=42
)
model.fit(X_train, y_train)

# 8. Оцениваем модель на валидационной выборке
y_val_pred = model.predict(X_val)
val_accuracy = accuracy_score(y_val, y_val_pred)
print(f"Точность модели на валидационной выборке: {val_accuracy:.2f}")

# 9. Визуализация
# 9.1 Матрица ошибок
conf_matrix = confusion_matrix(y_val, y_val_pred)
plt.figure(figsize=(8, 6))
sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', cbar=False, 
            xticklabels=np.unique(y), yticklabels=np.unique(y))
plt.title('Матрица ошибок')
plt.xlabel('Предсказанные метки')
plt.ylabel('Истинные метки')
plt.show()

# 9.2 Иерархическая диаграмма случайного леса
plt.figure(figsize=(15, 10))
plot_tree(model.estimators_[0], filled=True, feature_names=X.columns.tolist(), class_names=np.unique(y).astype(str).tolist())
plt.title('Иерархическая диаграмма первого дерева случайного леса')
plt.show()

# 9.3 Столбчатая диаграмма важности признаков
feature_importances = model.feature_importances_
indices = np.argsort(feature_importances)[::-1]

plt.figure(figsize=(10, 6))
plt.title('Важность признаков')
plt.bar(range(X.shape[1]), feature_importances[indices], align='center')
plt.xticks(range(X.shape[1]), X.columns[indices], rotation=90)
plt.xlim([-1, X.shape[1]])
plt.show()

# 9.4 ROC-кривая
y_val_prob = model.predict_proba(X_val)[:, 1]
fpr, tpr, _ = roc_curve(y_val, y_val_prob)
roc_auc = auc(fpr, tpr)

plt.figure(figsize=(8, 6))
plt.plot(fpr, tpr, color='blue', lw=2, label=f'ROC-кривая (area = {roc_auc:.2f})')
plt.plot([0, 1], [0, 1], color='red', lw=2, linestyle='--')
plt.title('ROC-кривая')
plt.xlabel('Ложноположительная доля')
plt.ylabel('Истинноположительная доля')
plt.legend(loc='lower right')
plt.show()

# 9.5 Гистограмма распределения признаков
plt.figure(figsize=(12, 8))
X.hist(bins=30, figsize=(15, 10), layout=(4, 4))
plt.suptitle('Распределение признаков', fontsize=16)
plt.show()

# 10. Вводим признаки для классификации
input_features = input("Введите признаки для классификации, разделенные запятыми: ").split(',')
try:
    input_features = [float(i) for i in input_features]  # Преобразуем в числовой формат
except ValueError:
    print("Ошибка: убедитесь, что все введенные признаки являются числовыми.")
    exit()

# 11. Масштабируем введенные признаки
input_features_scaled = scaler.transform(pd.DataFrame([input_features]))

# 12. Прогнозируем класс на основе введенных признаков
predicted_class = model.predict(input_features_scaled)[0]  # Предсказанное значение

# 13. Выводим предсказанное значение
print(f"Предсказанный класс на введенных признаках: {predicted_class}")
