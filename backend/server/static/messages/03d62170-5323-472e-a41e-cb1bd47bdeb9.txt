import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, roc_curve, auc
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np
from sklearn.tree import plot_tree

# 1. Считываем файл Excel
file_path = input("Введите путь к файлу Excel: ")

# Проверка на наличие файла
try:
    data = pd.read_excel(file_path)
    
    # Выводим количество строк и столбцов
    print(f"Количество строк: {data.shape[0]}")
    print(f"Количество столбцов: {data.shape[1]}")
except Exception as e:
    print(f"Ошибка при загрузке файла: {e}")
    exit()

# 2. Вводим номера столбцов для признаков и целевой переменной
features_index = input("Введите номера столбцов признаков, разделенные запятыми: ").split(',')
target_index = input("Введите номер столбца целевой переменной: ")

# Преобразуем индексы в целые числа и извлекаем данные
try:
    features_index = [int(i) for i in features_index]
    X = data.iloc[:, features_index]
    y = data.iloc[:, int(target_index)]
except (ValueError, IndexError) as e:
    print(f"Ошибка при выборе столбцов: {e}")
    exit()

# Проверка на наличие пустых значений
if X.isnull().any().any() or y.isnull().any():
    print("Данные содержат пустые значения. Пожалуйста, очистите данные и попробуйте снова.")
    exit()

# Проверка распределения классов
print("Распределение классов в целевой переменной:")
print(y.value_counts())

# 3. Масштабируем данные
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# 4. Делим данные на обучающую, валидационную и тестовую выборки
train_size = float(input("Введите долю обучающей выборки (например, 0.6 для 60%): "))
val_size = float(input("Введите долю валидационной выборки (например, 0.2 для 20%): "))
test_size = 1 - train_size - val_size

X_train, X_temp, y_train, y_temp = train_test_split(X_scaled, y, test_size=test_size, random_state=42)
X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)

# 5. Запрашиваем параметры для модели
n_estimators = int(input("Введите количество деревьев в лесу: "))
max_depth = int(input("Введите максимальную глубину дерева: "))
min_samples_split = int(input("Введите минимальное количество образцов для разбиения узла: "))
max_samples = float(input("Введите максимальное количество образцов в дереве (например, 0.8 для 80%): "))

# 6. Обучаем модель Случайный лес
model = RandomForestClassifier(
    n_estimators=n_estimators,
    max_depth=max_depth,
    min_samples_split=min_samples_split,
    max_samples=max_samples,
    random_state=42
)
model.fit(X_train, y_train)

# 7. Оцениваем модель на валидационной выборке
y_val_pred = model.predict(X_val)
val_accuracy = accuracy_score(y_val, y_val_pred)
print(f"Точность модели на валидационной выборке: {val_accuracy:.2f}")

# 8. Визуализация
# 8.1 Матрица ошибок
conf_matrix = confusion_matrix(y_val, y_val_pred)
plt.figure(figsize=(8, 6))
sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', cbar=False, 
            xticklabels=np.unique(y), yticklabels=np.unique(y))
plt.title('Матрица ошибок')
plt.xlabel('Предсказанные метки')
plt.ylabel('Истинные метки')
plt.show()

# 8.2 Иерархическая диаграмма случайного леса
plt.figure(figsize=(15, 10))
plot_tree(model.estimators_[0], filled=True, feature_names=X.columns.tolist(), class_names=np.unique(y).astype(str).tolist())

plt.title('Иерархическая диаграмма первого дерева случайного леса')
plt.show()

# 8.3 Столбчатая диаграмма важности признаков
feature_importances = model.feature_importances_
indices = np.argsort(feature_importances)[::-1]

plt.figure(figsize=(10, 6))
plt.title('Важность признаков')
plt.bar(range(X.shape[1]), feature_importances[indices], align='center')
plt.xticks(range(X.shape[1]), X.columns[indices], rotation=90)
plt.xlim([-1, X.shape[1]])
plt.show()

# 8.4 ROC-кривая
y_val_prob = model.predict_proba(X_val)[:, 1]
fpr, tpr, _ = roc_curve(y_val, y_val_prob)
roc_auc = auc(fpr, tpr)

plt.figure(figsize=(8, 6))
plt.plot(fpr, tpr, color='blue', lw=2, label=f'ROC-кривая (area = {roc_auc:.2f})')
plt.plot([0, 1], [0, 1], color='red', lw=2, linestyle='--')
plt.title('ROC-кривая')
plt.xlabel('Ложноположительная доля')
plt.ylabel('Истинноположительная доля')
plt.legend(loc='lower right')
plt.show()

# 8.5 Гистограммы плотности
plt.figure(figsize=(12, 8))
for feature in X.columns:
    sns.kdeplot(data[feature], label=feature, fill=True)
plt.title('Гистограммы плотности признаков')
plt.xlabel('Значения признаков')
plt.ylabel('Плотность')
plt.legend()
plt.show()

# 9. Вводим признаки для классификации
input_features = input("Введите признаки для классификации, разделенные запятыми: ").split(',')
try:
    input_features = [float(i) for i in input_features]  # Преобразуем в числовой формат
except ValueError:
    print("Ошибка: убедитесь, что все введенные признаки являются числовыми.")
    exit()

# 10. Масштабируем введенные признаки
input_features_scaled = scaler.transform(pd.DataFrame([input_features]))

# 11. Прогнозируем класс на основе введенных признаков
predicted_class = model.predict(input_features_scaled)[0]  # Предсказанное значение

# 12. Выводим предсказанное значение
print(f"Предсказанный класс на введенных признаках: {predicted_class}")
#C:/Users/USER/Desktop/library/exper.xlsx